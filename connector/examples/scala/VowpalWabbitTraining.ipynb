{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "*Licensed under the MIT License.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VW MMLSpark Model for AI on Accumulo\n",
    "\n",
    "\n",
    "In this notebook, we train a Vowpal Wabbit model in MMLSpark using [sentiment140](http://help.sentiment140.com/for-students/?source=post_page---------------------------) twitter data. [Microsoft Accumulo Spark Connector (MASC)](https://github.com/microsoft/masc) is used for handling data IO between Accumulo and Spark.   \n",
    "\n",
    "Before running this notebook, please\n",
    "* make sure you have Accumulo 2.0.0 and Spark 2.4.3 installed\n",
    "* create and activate a conda environment with Apache Toree installed\n",
    "* download accumulo-spark-datasource jar and accumulo-spark-iterator jar\n",
    "* run commands like the following to install a Jupyter toree kernel\n",
    "```\n",
    "# Replace the jar file path based on your situation\n",
    "JAR=\"file:///home/rba1/twitter-sentiment/lib/accumulo-spark-datasource-1.0.0-SNAPSHOT-shaded.jar\"\n",
    "jupyter toree install \\\n",
    "    --replace \\\n",
    "    --user \\\n",
    "    --kernel_name=accumulo \\\n",
    "    --spark_home=${SPARK_HOME} \\\n",
    "    --spark_opts=\"--master yarn --jars $JAR \\\n",
    "        --packages org.apache.spark:spark-avro_2.11:2.4.3,com.microsoft.ml.spark:mmlspark_2.11:0.18.1 \\\n",
    "        --driver-memory 8g \\\n",
    "        --executor-memory 6g \\\n",
    "        --driver-cores 2 \\\n",
    "        --executor-cores 2 \\\n",
    "        --num-executors 16\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version 2.4.3\n",
      "Scala version 2.11.12\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.eventLog.enabled,true)\n",
      "(spark.repl.local.jars,file:///home/rba1/twitter-sentiment/lib/accumulo-spark-datasource-1.0.0-SNAPSHOT-shaded.jar,file:///home/rba1/.ivy2/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar,file:///home/rba1/.ivy2/jars/com.microsoft.ml.spark_mmlspark_2.11-0.18.1.jar,file:///home/rba1/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///home/rba1/.ivy2/jars/org.scalactic_scalactic_2.11-3.0.5.jar,file:///home/rba1/.ivy2/jars/org.scalatest_scalatest_2.11-3.0.5.jar,file:///home/rba1/.ivy2/jars/io.spray_spray-json_2.11-1.3.2.jar,file:///home/rba1/.ivy2/jars/com.microsoft.cntk_cntk-2.4.jar,file:///home/rba1/.ivy2/jars/org.openpnp_opencv-3.2.0-1.jar,file:///home/rba1/.ivy2/jars/com.jcraft_jsch-0.1.54.jar,file:///home/rba1/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.6.jar,file:///home/rba1/.ivy2/jars/com.microsoft.ml.lightgbm_lightgbmlib-2.2.350.jar,file:///home/rba1/.ivy2/jars/com.github.vowpalwabbit_vw-jni-8.7.0.2.jar,file:///home/rba1/.ivy2/jars/org.scala-lang_scala-reflect-2.11.12.jar,file:///home/rba1/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.6.jar,file:///home/rba1/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.10.jar,file:///home/rba1/.ivy2/jars/commons-logging_commons-logging-1.2.jar,file:///home/rba1/.ivy2/jars/commons-codec_commons-codec-1.10.jar)\n",
      "(spark.repl.class.uri,spark://rbaaccucluster2-0:44299/classes)\n",
      "(spark.driver.host,rbaaccucluster2-0)\n",
      "(spark.eventLog.dir,hdfs://rbaaccucluster2/spark/history)\n",
      "(spark.app.name,TwitterSentimentClassification)\n",
      "(spark.jars,file:/home/rba1/.local/share/jupyter/kernels/accumulo_scala/lib/toree-assembly-0.3.0-incubating.jar)\n",
      "(spark.executor.id,driver)\n",
      "(spark.executor.instances,16)\n",
      "(spark.driver.appUIAddress,http://rbaaccucluster2-0:4040)\n",
      "(spark.executor.cores,2)\n",
      "(spark.driver.port,44299)\n",
      "(spark.executor.memory,6g)\n",
      "(spark.master,yarn)\n",
      "(spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS,rbaaccucluster2-0)\n",
      "(spark.yarn.dist.jars,file:///home/rba1/twitter-sentiment/lib/accumulo-spark-datasource-1.0.0-SNAPSHOT-shaded.jar,file:///home/rba1/.ivy2/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar,file:///home/rba1/.ivy2/jars/com.microsoft.ml.spark_mmlspark_2.11-0.18.1.jar,file:///home/rba1/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///home/rba1/.ivy2/jars/org.scalactic_scalactic_2.11-3.0.5.jar,file:///home/rba1/.ivy2/jars/org.scalatest_scalatest_2.11-3.0.5.jar,file:///home/rba1/.ivy2/jars/io.spray_spray-json_2.11-1.3.2.jar,file:///home/rba1/.ivy2/jars/com.microsoft.cntk_cntk-2.4.jar,file:///home/rba1/.ivy2/jars/org.openpnp_opencv-3.2.0-1.jar,file:///home/rba1/.ivy2/jars/com.jcraft_jsch-0.1.54.jar,file:///home/rba1/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.6.jar,file:///home/rba1/.ivy2/jars/com.microsoft.ml.lightgbm_lightgbmlib-2.2.350.jar,file:///home/rba1/.ivy2/jars/com.github.vowpalwabbit_vw-jni-8.7.0.2.jar,file:///home/rba1/.ivy2/jars/org.scala-lang_scala-reflect-2.11.12.jar,file:///home/rba1/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.6.jar,file:///home/rba1/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.10.jar,file:///home/rba1/.ivy2/jars/commons-logging_commons-logging-1.2.jar,file:///home/rba1/.ivy2/jars/commons-codec_commons-codec-1.10.jar)\n",
      "(spark.yarn.secondary.jars,accumulo-spark-datasource-1.0.0-SNAPSHOT-shaded.jar,org.apache.spark_spark-avro_2.11-2.4.3.jar,com.microsoft.ml.spark_mmlspark_2.11-0.18.1.jar,org.spark-project.spark_unused-1.0.0.jar,org.scalactic_scalactic_2.11-3.0.5.jar,org.scalatest_scalatest_2.11-3.0.5.jar,io.spray_spray-json_2.11-1.3.2.jar,com.microsoft.cntk_cntk-2.4.jar,org.openpnp_opencv-3.2.0-1.jar,com.jcraft_jsch-0.1.54.jar,org.apache.httpcomponents_httpclient-4.5.6.jar,com.microsoft.ml.lightgbm_lightgbmlib-2.2.350.jar,com.github.vowpalwabbit_vw-jni-8.7.0.2.jar,org.scala-lang_scala-reflect-2.11.12.jar,org.scala-lang.modules_scala-xml_2.11-1.0.6.jar,org.apache.httpcomponents_httpcore-4.4.10.jar,commons-logging_commons-logging-1.2.jar,commons-codec_commons-codec-1.10.jar)\n",
      "(spark.ui.proxyBase,/proxy/application_1574703914462_0013)\n",
      "(spark.history.fs.logDirectory,hdfs://rbaaccucluster2/spark/history)\n",
      "(spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES,http://rbaaccucluster2-0:8088/proxy/application_1574703914462_0014)\n",
      "(spark.driver.memory,8g)\n",
      "(spark.app.id,application_1574703914462_0014)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.ui.filters,org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter)\n",
      "(spark.yarn.historyServer.address,rbaaccucluster2-0:18080)\n",
      "(spark.repl.class.outputDir,/tmp/spark-d2e52880-61e4-46e6-a918-5e2a333695e1/repl-ddfc57e1-7aeb-43e0-8606-db2f241d25cf)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "conf = org.apache.spark.SparkConf@3440396b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkConf@3440396b"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "// Stop existing spark context and create new one\n",
    "sc.stop()\n",
    "\n",
    "val conf = new SparkConf()\n",
    "conf.setAppName(\"TwitterSentimentClassification\")\n",
    "\n",
    "new SparkContext(conf)\n",
    "\n",
    "println(\"Spark version %s\".format(sc.version))\n",
    "println(\"Scala %s\".format(util.Properties.versionString))\n",
    "println\n",
    "sc.getConf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROPS_PATH = /home/rba1/install/accumulo-2.0.0/conf/accumulo-client.properties\n",
       "TRAIN_TABLE_NAME = twitter_train_data\n",
       "sqlContext = org.apache.spark.sql.SQLContext@74dbb18b\n",
       "schema = StructType(StructField(sentiment,DoubleType,true), StructField(id,StringType,true), StructField(date,StringType,true), StructField(query_string,StringType,true), StructField(user,StringType,true), StructField(text,StringType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(sentiment,DoubleType,true), StructField(id,StringType,true), StructField(date,StringType,true), StructField(query_string,StringType,true), StructField(user,StringType,true), StructField(text,StringType,true))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{LongType, DoubleType, StringType, StructField, StructType}\n",
    "import org.apache.accumulo.core.client.Accumulo\n",
    "import scala.collection.JavaConverters._\n",
    "\n",
    "// client property file path\n",
    "val PROPS_PATH = \"/home/rba1/install/accumulo-2.0.0/conf/accumulo-client.properties\"\n",
    "val TRAIN_TABLE_NAME = \"twitter_train_data\"\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "val schema = StructType(Array(\n",
    "    StructField(\"sentiment\", DoubleType),\n",
    "    StructField(\"id\", StringType),\n",
    "    StructField(\"date\", StringType),\n",
    "    StructField(\"query_string\", StringType),\n",
    "    StructField(\"user\", StringType),\n",
    "    StructField(\"text\", StringType)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Twitter Data to Accumulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to ingest twitter data to Accumulo: 22.002847971s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file_path = sentiment140_prefix.csv\n",
       "df = [sentiment: double, id: string ... 4 more fields]\n",
       "t0 = 18420090028815\n",
       "props = {auth.type=password, auth.principal=root, table=twitter_train_data, instance.zookeepers=rbaaccucluster2-0:2181,rbaaccucluster2-1:2181,rbaaccucluster2-2:2181, instance.name=muchos, rowKey=id, auth.token=secret}\n",
       "t1 = 18442092876786\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "18442092876786"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// need to upload data to hdfs first via \n",
    "// hdfs dfs -put /home/rba1/twitter-sentiment/sentiment140_prefix.csv sentiment140_prefix.csv\n",
    "val file_path = \"sentiment140_prefix.csv\"\n",
    "val df = spark.read.format(\"csv\").schema(schema).load(file_path)\n",
    "\n",
    "var t0 = System.nanoTime()\n",
    "val props = Accumulo.newClientProperties().from(PROPS_PATH).build()\n",
    "props.put(\"table\", TRAIN_TABLE_NAME)\n",
    "props.put(\"rowKey\", \"id\")\n",
    "df.write.format(\"org.apache.accumulo\").options(props.asScala).save()\n",
    "var t1 = System.nanoTime()\n",
    "println(\"Time to ingest twitter data to Accumulo: \" + (t1 - t0)*1e-9 + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data from Accumulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data from Accumulo...\n",
      "Time to load training data: 34.577519024000004s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 18443853066868\n",
       "train_df = [sentiment: double, id: string ... 5 more fields]\n",
       "t1 = 18478430585892\n",
       "read_time = 34.577519024000004\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "34.577519024000004"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Reading training data from Accumulo...\")\n",
    "var t0 = System.nanoTime()\n",
    "var train_df = spark.read\n",
    "                    .format(\"org.apache.accumulo\")\n",
    "                    .options(props.asScala)\n",
    "                    .schema(schema)\n",
    "                    .load()\n",
    "train_df.cache().count()\n",
    "var t1 = System.nanoTime()\n",
    "val read_time = (t1 - t0)*1e-9\n",
    "println(\"Time to load training data: \" + read_time + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_df = [label: double, text: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, text: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{rand, when}\n",
    "\n",
    "train_df = train_df.orderBy(rand()) // Randomly permute the data for online training\n",
    "                   .withColumn(\"label\", 'sentiment.cast(\"Int\"))\n",
    "                   .select('label as 'label, 'text as 'text)\n",
    "                   .withColumn(\"label\", when('label > 0, 1.0D).otherwise(-1.0D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vwFeaturizer = VowpalWabbitFeaturizer_f1627b144bf8\n",
       "vwParams = --loss_function=logistic --quiet --holdout_off\n",
       "vw = VowpalWabbitClassifier_62338413e2e0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VowpalWabbitClassifier_62338413e2e0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import com.microsoft.ml.spark.vw.{VowpalWabbitFeaturizer, VowpalWabbitClassifier}\n",
    "\n",
    "val vwFeaturizer = new VowpalWabbitFeaturizer()\n",
    " .setStringSplitInputCols(Array(\"text\"))\n",
    " .setOutputCol(\"features\")\n",
    "\n",
    "val vwParams = \"--loss_function=logistic --quiet --holdout_off\"\n",
    "val vw = new VowpalWabbitClassifier()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setArgs(vwParams)\n",
    "    .setNumPasses(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train Vowpal Wabbit model: 19.832663679s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vw_pipeline = pipeline_3d0899e64c3e\n",
       "t0 = 18482474609412\n",
       "vwModel = pipeline_3d0899e64c3e\n",
       "t1 = 18502307273091\n",
       "train_time = 19.832663679\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "19.832663679"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define a training pipeline\n",
    "val vw_pipeline = new Pipeline().setStages(Array(vwFeaturizer, vw))\n",
    "\n",
    "var t0 = System.nanoTime()\n",
    "val vwModel = vw_pipeline.fit(train_df)\n",
    "var t1 = System.nanoTime()\n",
    "val train_time = (t1 - t0)*1e-9\n",
    "println(\"Time to train Vowpal Wabbit model: \" + train_time + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Save model to hdfs\n",
    "vwModel.write.overwrite().save(\"./model/vwModel_twitter_sentiment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accumulo - Scala",
   "language": "scala",
   "name": "accumulo_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
